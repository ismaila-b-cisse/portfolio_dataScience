Cette partie se concentre sur l'extraction des données, notamment le web scraping. Il s'agit d'extraire les données d'un site web. Cette extraction peut être de manière automatisée. Sur ce type d'extraction, souvent des questions légales et éthiques se posent. Ainsi, est-il légal de faire du scraping sur un site web ? J'ai trouvé les réponses à cette question de ces trois liens très intéressantes. L'un c'est [Le Web Scraping est-il légal ?](https://www.app.asso.fr/preuve-digitale/web-scraping-legal.html) qui affirme que "l’activité de web scraping n’est absolument pas illicite ou interdite en soi", néanmoins, il faut tenir compte de quatre points :
    - le respect du droit sui generis - doit d'auteur
    - le respect des conditions générales d'utilisation (cgu) - droit des contrats
    - le respect des lois en matière d'accès et de maintien dans un STAD (système de traitement automatisé de données) - droit pénal
    - le respect des principes du RGPD lors du traitement de données - droit de la protection des données
L'autre, [Le web scraping est-il illégal ? Comprendre les enjeux juridiques](https://thunderbit.com/fr/blog/web-scraping-legal-implications).
Enfin sur le dernier lien, un article du CNIL, [La base légale de l’intérêt légitime : fiche focus sur les mesures à prendre en cas de collecte des données par moissonnage (web scraping)](https://www.cnil.fr/fr/focus-interet-legitime-collecte-par-moissonnage) 

Dans le cadre de ce portfolio, avant de faire du scraping sur un site donnée, j'analyse le fichiers robots.txt pour le côté technique et les cgu du site. Je fais primer ces derniers sur celui de robots.txt. Ainsi, pour faciliter leur analyse, j'ai implémenté deux modules respectivement robots_text et cgu dans le dossier ethic dont l'un content une fonction qui prend la base de l'url du site web et un ensemble de mots clé pour vérifier si ces derniers se trouve ou non dans le fichier robot.txt. L'autre contient une fonction qui prend la base de l'url, le chemin relatif des cgu et une liste de mots clés à chercher dans les cgu et indique les passages des mots clés si elle les trouvent. Dans une seconde version de cette fonction, elle prend l'url des cgu et la liste de mots clés et indiquent s'ils s'y trouve et dans quel passage avec le titre.

Ensuite, après avoir pris connaissance du fichier robots.txt et des cgu des sites sur lesquels je souhaite faire le scraping, j'ai par la suite, contacté les éditeurs du site par téléphone, mail, sur linkedIn, parfois je me suis déplacé dans certaines de leurs boutiques à leur demande. Cependant, je n'ai pas reçu de réponses de leur part concernant l'objet de ma demande. Néanmoins, j'ai reçu une réponse d'amazon qui n'est une autorisation explicite mais qui ne me l'interdit pas au vu de l'objet de l'extraction.  Ainsi, je me suis limité à l'extraction des données publiquement accessibles, avec une extraction limité pour les besoin de ce projet.
Les données que nous allons extraire alors dans le cadre de ce portfolio sont public et ne sont pas des données personnelles. Ce projet, qui n'a pas de but commercial, consite à illustrer les techniques d'extraction, nettoyage, d'analyse et de modélisation de données.

Après cette phase de l'extraction, j'ai implémenté, pour l'instant, trois modules dans le dossier scraping : temu_scraping, darty_scraping et scraper.
Enfin, j'ai implémenté le main dans lequel j'importe les modules robots_text, cgu et scraper. Ce dernier permet d'extraire les données dont nous avons besoin pour l'analyse. Mais, avant cela, nous avons besoin de les nettoyer et les transformer.
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4cf79b-92ba-4c6c-8e5f-5ab7d5e8b163",
   "metadata": {},
   "source": [
    "# Extraction des données\n",
    "\n",
    "Dans le vaste domaine de la data ou des données, il y a toujours une phase qui est indispensable : c'est avoir les données. Or, pour avoir les données, il faut les avoir quelque part. Soit recueillir directement auprès des utilisateurs, consommateurs, soit à partir d'une base de données que vous aviez déjà mise en place, soit en seconde main... \n",
    "Notre objectif est d'extraire des données sur au moins un site web. Cette extraction concerne des produits sur différents sites e-commerce pour un but pédagogique et non commercial.\n",
    "\n",
    "Dans ce projet, nous nous proposons d'étudier les produits des site e-commerces. Nous voulons étudier entre autre quel produit a le prix le plus avantageux pour le consommateur suivant les sites, les tendances des produits, le prix d'un produit selon qu'il soit en promotion ou non. je fais l'extraction des données. \n",
    "\n",
    "Pour ce faire, nous avons besoin de passer la première étape de l'analyse de données qui est avoir les données. Or, pour avoir les données, il faut les avoir quelque part. Soit les collecter soi-même, soit elles sont dans des bases de données, dans des datawarehous, dans ce cas on peut faire un dump (à expliquer), soit les récupérer sur des sites dans fichiers csv. Tout cela a un nom qui est l'extraction de données.\n",
    "\n",
    "L'extraction des données est un processus qui permet de collecter des données sur divers sources dans un but de les stocker ou des les traiter pour plus tard. Les données peuvent être publiques ou privées. Ainsi, les données peuevnt être soumis à des licences publiques ou privées. Les sources de données peuvent très nombreuses, comme des site gouvernementaux, sur des site comme google, des plateformes comme kaggle, avec des licences publiques...\n",
    "\n",
    "Il y a différents techniques de collecter ou d'extraire des données, comme le web scraping, qui diffèrent du simple récupération d'un fichier csv sur un site. Dans ce projet, c'est justement le web scraping que nous allons utiliser pour récupérer des données sur des sites e-commerces. \n",
    "\n",
    "Nous voulons faire ce qui est communément appelé par le web scraping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5762130-4e94-4592-ad72-a856f208dfd8",
   "metadata": {},
   "source": [
    "## Web Scraping et éthique\n",
    "\n",
    "Le web-scraping est une technique d'extraction de données, un traitement automatisé qui consiste à collecter des données sur un site web. Alors, il faut préciser d'emblée que tous les site web n'autorisent pas la collecte de leurs données. D'où l'importance de vérifier la politique du site web avant de procéder à la collecte de ses données pour savoir s'il autorise ou non le web scraping. \n",
    "D'abord, j'ai souhaité inclure Amazon, car étant l'un des site de vente les plus populaire, et donc pouvant donner la possibilité d'avoir beaucoup données, dans les sites sur lequels je collecte des données, mais après des recherches si il autorise ou pas le web scraping, je me suis rendu compte que partiquement pour l'écrasante majorité des attributs que nous avons besoin, il n'autorise pas les web scraping.  On peut voir ces chose en consultant le robot.txt du site et les conditions générales d'utilisation (CGU). \n",
    "\n",
    "### Analyse des fichiers du site\n",
    "#### le fichier robots\n",
    "Si vous voulez, ce fichier est comme un panneau de signalisation indiquant les chemins à ne pas prendre, parfois ce panneau concerne toute voiture, parfois est spécifique à certaines voiture, par exemple les poids lourds.\n",
    "Par exemple, pour Amazon, j'ai consulté https://www.amazon.com/robots.txt. S'il y a des parties que vous ne comprenez pas. C'est tout à fait normal, vous pouvez vous avec chatGPT. J'ai également regardé leurs conditions générales et d'utilisation, en cherchant avec les mot clés \"robot\" ou \"collecte\", il ne l'autorise également pas sans un accord express et écrit. J'ai également utilisé chatGPT pour voir si ses résultats correspondent à certains de mes résultats.\n",
    "\n",
    "Finalement, j'ai me suis dirigé vers d'autres site qui permettent cela et j'ai inclus aussi pour avoir plus des données des API. Ainsi, \n",
    "\n",
    "#### Les conditions générales d'utilisation (CGU)\n",
    "Pour les CGU, j'ai commencé à lire les CGU des sites qui peuvent m'intéresser dans le cadre de ce projet, notamment les parties susceptibles de contenir les passages que je cherche, en particulier ceux sur le web scraping. J'utilise parfois chatGPT pour qu'il interprète certains passages ou qu'il aille chercher les CGU du site, sur lequel je souhaite approfondir mes connaissances sur ces CGU, et de porter une attention particulière à certains mots clés. Il a fait de belles choses. Mais, comme généralement, avec les IA génératives, il ne faut pas s'empêcher d'aller vérifier certaines de leurs informations. Ainsi, s'il me donne son résumé sur ces passages, je peux aller regarder les CGU du site concerné, une fois que je sais que ce passage y existerait, vérifiant en même temps ce qui y est dit exactement. C'est de cette manière que j'ai procédé avec les CGU du site temu.Toujours, dans le cadre de l'extraction des données sur ce projet, en particulier dans celui du web scraping, j'ai voulu faciliter la tâche en faisant un programme qui analyse les CGU du site sur lequel je souhaite faire du scraping en cherchant des mots clés et des passages qui autorisent ou non le web scraping, comme celui sur le fichier robots du site. Contrairement au programme sur le fichier robots.txt où l'utilisateur ne donne que le domaine du site et une liste de mots clés, dans celui pour les CGU, il doit fournir le domaine du site et le chemin relative des CGU et une liste de mots clés. Je l'ai fait ainsi parce qu'on n'a pas les mêmes chemins relatifs des CGU selon les sites, ni les mêmes nomination. Parfois, c'est/CGU, parfois /policies, sur d'autres sites c'est /help/... Les mots clés sont ce que j'ai jugé au travers des lectures de CGU mais j'ai également utilisé chatGPT pour les mots clés qui sont plus pertinents à rechercher dans les CGU concernant le web scraping. Enfin, j'ai fais un tri pour choisir les plus pertinents de ceux issus des lectures de CGU et ceux proposés par chatGPT.Donc, si l'utilisateur fournit ces deux chemins et la liste des mots à cherché, le programme analyse le contenue identifie la présence de ses mots ainsi que leurs passage. S'il les trouve, il les affiche. Ce programme permet de faciliter la recherche de certains mots clés, des passages qui doivent attirer l'attention de celui qui souhaite scraper un site. Cependant, la non exhaustivité des termes de la liste peut être une limite pour ce programme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada5e8d-8019-46a9-b2b3-7ff973a343a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4d045e-4a72-4a7a-a1de-8f3b7661c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import mechanicalsoup\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "#from playwright.sync_api import sync_playwright\n",
    "from playwright.async_api import async_playwright, TimeoutError\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "import asyncio\n",
    "#import re\n",
    "from rich import print\n",
    "import json\n",
    "import csv\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28f8e5a-9df0-4a81-8566-be1108885feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Préalables pour le web scraping\n",
    "# analyse du fichier robots\n",
    "def robots_file_analysis(base_url):\n",
    "    \"\"\"\n",
    "        Le fichier robots.txt contient des chemins que les bots ou robots\n",
    "        ne doivent pas utiliser. Ce paneau d'attention est parfois addressé\n",
    "        à tout robot, parfois à des robots spécifiques pour leur dire de ne pas\n",
    "        emprunter ces chemin. Ainsi, ici, j'analyse les chemins que le site ne veulent\n",
    "        pas mon programme emprunte pour y extraire des données. Ces chemins contiennent des \n",
    "        mots clés qui nous intéressent dans notre scraping. S'ils y sont présent, et que c'est \n",
    "        qu'ils sont précédés par un \"Disallow\", on pourrait ne pas estraire ces données. En revanche, \n",
    "        s'ils sont précédés par un \"Allow\", alors on pourrait les extraire de ce site.\n",
    "        Par exemple, dans notre présent projet, nous voulons extraire du site les produits et certains de \n",
    "        leur caractérisques. ALors \"produit\" est un mot clé pour nous. Si un chemin contient ce mot clé dans \n",
    "        le fichier robots.txt, alors notre programme le détecte et l'affiche. Une fois qu'on a ce chemin, on peut\n",
    "        voir s'il s'agit de tous les produits ou certains de ces caractéristiques.\n",
    "        \n",
    "        Cette fonction --robots_file_analysis-- prend l'URL de base du site qu'on souhaite extraire ces données, on y ajoute \n",
    "        le chemin vers le fichier \"/robots.txt\" qu'on souhaite analyser, et renvoie un dictionnaire\n",
    "        contenant les user-agent, comme clés, et les chemins comme valeurs.\n",
    " \n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\"user-agnet\":\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:143.0) Gecko/20100101 Firefox/143.0\"}\n",
    "\n",
    "    #response = requests.get(\"https://httpbin.io/user-agent\")\n",
    "    # Envoi d'une requête au site\n",
    "    response = requests.get(base_url+\"/robots.txt\", headers=headers)\n",
    "\n",
    "    # On construit une liste des lignes du fichier robots\n",
    "    robots_lines_list = response.text.lower().splitlines()\n",
    "    #print(robots_text_lines)\n",
    "    \n",
    "    # On récupère tous les user-agent du robots.txt\n",
    "    userAgent_list = []\n",
    "    for i, agent in enumerate(robots_lines_list):\n",
    "        userAgent_indice = agent.find(\"user-agent\")\n",
    "        if userAgent_indice != -1:\n",
    "            userAgent_list.append(agent)\n",
    "    # print(userAgent_list)\n",
    "    \n",
    "    # On construit un dictionnaire avec comme key le user-agent et value\n",
    "    # l'item qui ne doit être aspiré\n",
    "    robots_text_dict = {}\n",
    "    # On construit un dictionnaire avec comme key le user-agent et value\n",
    "    # l'item qui ne doit être aspiré\n",
    "    for i, userAgent in enumerate(userAgent_list):\n",
    "        start = robots_lines_list.index(userAgent)+1\n",
    "        \n",
    "        if i+1 < len(userAgent_list):\n",
    "            end = robots_lines_list.index(userAgent_list[i+1])\n",
    "            robots_text_dict[userAgent] = robots_lines_list[start:end]\n",
    "        else:\n",
    "            robots_text_dict[userAgent] = robots_lines_list[start:]\n",
    "    \n",
    "    return robots_text_dict\n",
    "\n",
    "\n",
    "def robots_results(base_url, robots_keywords_list):\n",
    "    \"\"\"\n",
    "        Cette fonction -- robots_results -- appelle la --robots_file_analysis--. Elle aussi prend \n",
    "        l'URL de base du site, avec le chemin robots.txt, et une liste de mots clés. Elle renvoie \n",
    "        les résultats dans un dictionnaire contenant les user-agent, comme clés, concatenés avec le mot clé recherché, \n",
    "        séparés par un \"_\", et comme valeurs les chemins trouvés dans lesquels il y a le mot clé qu'on\n",
    "        recherche.       \n",
    "        \n",
    "        J'ai paramétré le mots clés, car nous ne chercons pas forcément à extraire les mêmes choses. \n",
    "        Ainsi, l'utilisateur peut mettre les mots sur lesquels il souhaite travailler. Pour moi, par exemple,\n",
    "        parmi les mots qui m'interresse figurent \"produit\", \"product\", \"catalogue\", ...\n",
    "    \"\"\"\n",
    "    results_dic = robots_file_analysis(base_url)\n",
    "    \n",
    "    results = {}\n",
    "    for i, item in enumerate(results_dic.items()):\n",
    "        # On affiche les keywords présent dans les values de la clé\n",
    "        for robots_keyword in robots_keywords_list:\n",
    "            for j, robots_keyword_path in enumerate(item[1]):\n",
    "                if robots_keyword in robots_keyword_path:\n",
    "                    results.setdefault((item[0]+\"_\"+robots_keyword).replace(\" \", \"\"), []).append(robots_keyword_path)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    return results\n",
    "    \n",
    "\n",
    "def display_robots_results(base_url, robots_keywords_list):\n",
    "    \n",
    "    \"\"\"\n",
    "        Cette méthode -- display_robots_results -- appelle la fonction -- robots_results -- \n",
    "        et permet d'afficher les résultats obtenus. Si aucun des mots clés n'est trouvé, elle\n",
    "        nous affiche le message \"Aucun de ces mots clés que vous cherchez n'est trouvé dans le fichier /robots.txt !\"\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dic = robots_results(base_url, robots_keywords_list)\n",
    "    \n",
    "    #print(results_dic)\n",
    "    #print(results_dic.keys())\n",
    "    if results_dic == {}:\n",
    "        print(\"Aucun de ces mots clés que vous cherchez n'est trouvé dans le fichier /robots.txt !\")\n",
    "    else :\n",
    "        userAgent_list = []\n",
    "        for key, values in results_dic.items():\n",
    "            userAgent_key = key.split(\"_\") # le key avec split à partir \"_\" -> liste de deux mots : user-agent et le mot clé qui est cherché \n",
    "            if userAgent_key[0] not in userAgent_list:\n",
    "                userAgent_list.append(userAgent_key[0])\n",
    "        print(userAgent_list)        \n",
    "           \n",
    "        for key, values in results_dic.items():\n",
    "            userAgent_key = key.split(\"_\")\n",
    "            \n",
    "            if userAgent_key[0] in userAgent_list:\n",
    "                print(f\"{userAgent_key[0]}\")\n",
    "                userAgent_list.remove(userAgent_key[0])\n",
    "                \n",
    "            print(f\"\\t {userAgent_key[1]} est bien présent dans : \")\n",
    "            \n",
    "            for i, robots_keyword_path in enumerate(values):\n",
    "                print(\"\\t\\t\", robots_keyword_path)\n",
    "                \n",
    "    # lien vers robots.txt et les CGU\n",
    "    print(\"Pour plus d'informations, vous pouvez consulter : \", base_url+\"/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a54d762-b4fb-4680-b7fe-5803870ff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse du fichier conditions générales d'utilisation (CGU)\n",
    "def cgu_analysis(base_url, cgu_path):\n",
    "    headers = {\"user-agnet\":\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:143.0) Gecko/20100101 Firefox/143.0\"}\n",
    "\n",
    "    # France\n",
    "    proxies = {\n",
    "    \"http\": \"https://89.168.33.135:3217\",\n",
    "    \"https\": \"https://89.168.33.135:3217\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # keywords = \"cgu\", \"cgv\", \"conditions\", \"utilisation\", \"vente\", \"terms\", \"terms\"\n",
    "    #                \"service\", \"privacy\", \"policy\", \"legal\", \"mention\"\n",
    "    print(base_url+cgu_path)\n",
    "    response = requests.get(base_url+cgu_path, headers=headers) # proxies=proxies\n",
    "    print(response.status_code)\n",
    "    #print(response.history)\n",
    "    #print(response.text)\n",
    "    #session = requests.Session()\n",
    "\n",
    "    #page = urlopen(base_url)\n",
    "    #html_text = page.read().decode(\"utf-8\")\n",
    "    #print(html_text) # avec amazon, on a cette erreur : \n",
    "    # TypeError: urlopen() got an unexpected keyword argument 'headers'\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    print(soup.get_text())\n",
    "\n",
    "\n",
    "    # \"propiété\", \"propriété intellectuelle\", \"licence\", \"licence d'accès\", \n",
    "    # \"licence d'usage\", \"accès aux données\", \"license\", \"data access\", \"usage personnel\",\n",
    "    # \"usage non personnel\", \"usage commercial\", \"usage non commercial\", \"partie substantielle\",\n",
    "    # \"partie non susbtantielle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4169a5-7bee-40e5-8b1e-d337a75988dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour supprimer les balises html\n",
    "def remove_tags(soup_find_all_list):\n",
    "    texts_list = []\n",
    "    for text in soup_find_all_list:\n",
    "        text = text.get_text(strip=True)\n",
    "        texts_list.append(text)\n",
    "\n",
    "    return texts_list\n",
    "\n",
    "\n",
    "async def cgu_analysis_auto(base_url, cgu_path, keywords_list):\n",
    "    \"\"\"\n",
    "        Cette fonction prend la base de l'url d'un site web, le chemin relatif\n",
    "        de l'url des conditions générales d'utilisation et une list de de mots clés,\n",
    "        pour vérifier si ces dernisers sont présents ou pas dans le contenu des cgu.\n",
    "        Elle récupère le contenu d'une page html, notamment les cgu, et retourne \n",
    "        un dictionnaire avec le mot clé et le paragraphe dans lequel il se trouve.\n",
    "    \"\"\"\n",
    "    async with async_playwright() as pw:\n",
    "        browser = await pw.chromium.launch(headless=False)\n",
    "        #browser = await pw.firefox.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "        \n",
    "        async def handle_response(response):\n",
    "            url = response.url\n",
    "            try:                \n",
    "                if \"api\" in url or url.endswith(\"json\"):\n",
    "                    data = await response.json\n",
    "                    print(\"API trouvé : \", url)\n",
    "                    print(data)\n",
    "            except:\n",
    "                pass\n",
    "                #print(\"Pas d'API trouvé !\")\n",
    "\n",
    "        # La page écoute l'évenenment \"response\", si le navigateur reçoit des réponses, \n",
    "        # elle appelle un callback en l'occurrence handle_response qui traite les responses reçues et \n",
    "        # les trie selon qu'ils soient .json ou non\n",
    "        #page.on(\"response\", handle_response)\n",
    "        page.on(\"response\", f=lambda response: handle_response(response))\n",
    "\n",
    "        \n",
    "        # La réponse de la navigation\n",
    "        response = await page.goto(base_url+cgu_path, wait_until=\"networkidle\")\n",
    "        #response = await page.goto(base_url+cgu_path, {timeout:0}, wait_until=\"networkidle\")\n",
    "        page.set_default_navigation_timeout(0)\n",
    "        status = response.status if response else None\n",
    "        print(\"Status HTTP : \", status)\n",
    "        \n",
    "        if status != 200:\n",
    "            print(\"Une erreur est survenue ! \\nVeuillez réssayer plus tard\" +\n",
    "            \"\\nou accédez au lien \", base_url+cgu_path)\n",
    "\n",
    "        #print(response.url)\n",
    "        #print(response.json)\n",
    "        await page.wait_for_selector(\"body\")\n",
    "        content = await page.content()\n",
    "        #print(content) # [:2000]\n",
    "\n",
    "        # On récupère le texte des cgu avec BeautifulSoup\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        #print(soup.get_text())\n",
    "        await asyncio.sleep(5)\n",
    "        #await browser.close()\n",
    "        \n",
    "        \n",
    "        # vérifier si un keyword est présent dans les titre ou sous-titres\n",
    "        # vérifier si un keyword est présent dans les paragraphes\n",
    "        h2_titles_list = soup.find_all(\"h2\")\n",
    "        #print(h2_titles_list)\n",
    "        h3_titles_list = soup.find_all(\"h3\")\n",
    "        p_list = soup.find_all(\"p\")\n",
    "        li_list = soup.find_all(\"li\")\n",
    "\n",
    "        # tags deletion\n",
    "        h2titles_list = remove_tags(h2_titles_list)\n",
    "        h3titles_list = remove_tags(h3_titles_list)\n",
    "        paragraphs_list = remove_tags(p_list)\n",
    "        puces_list = remove_tags(li_list)\n",
    "        \n",
    "\n",
    "        keywords_dic = {}\n",
    "        for keyword in keywords_list:\n",
    "            for paragraph in paragraphs_list:\n",
    "                if keyword in paragraph:                    \n",
    "                    keywords_dic.setdefault(keyword, []).append(paragraph)\n",
    "            for puce in puces_list:\n",
    "                if keyword in puce:\n",
    "                    keywords_dic.setdefault(keyword, []).append(puce)\n",
    "\n",
    "        return keywords_dic\n",
    "\n",
    "\n",
    "async def display_cgu_results(base_url, cgu_path, keywords_list):\n",
    "    \"\"\"\n",
    "        Affichage des mots clés trouvés dans les cgu de l'url et \n",
    "        leurs paragraphes\n",
    "    \"\"\"\n",
    "    keywords_dic = await cgu_analysis_auto(base_url, cgu_path, keywords_list)\n",
    "\n",
    "    # On met les mots clés qui sont présent dans le passage des cgu\n",
    "            # en gras et de couleur rouge \n",
    "    values_list = [] #list(keywords_dic.values())\n",
    "    for key, values in keywords_dic.items():\n",
    "        #print(type(values), \"-- \", values)\n",
    "        #print(\"***\",\"[bold]\"+key+\"[/bold]\", \" ---> \", values)\n",
    "        print(\"***\",\"[bold]\"+key+\"[/bold]\", \" ---> \")\n",
    "\n",
    "        for value in values:\n",
    "            value = value.replace(key, \"[bold red]\"+key+\"[/bold red]\")\n",
    "            print(\"\\t\", value)\n",
    "  \n",
    "    #     print()\n",
    "\n",
    "    print(\"Pour plus d'informations, vous pouvez consulter : \", base_url+cgu_path)\n",
    "\n",
    "    # print(\"Pour plus d'informations, vous pouvez consulter : \", base_url+cgu_path) \n",
    "\n",
    "\n",
    "# Si vous mettez un nombre important de à chercher, vous aurez beaucoup de passages. \n",
    "# Egalement, si vous mettez un mot clé seul qui est abondamment cité dans les cgu, \n",
    "# comme licence ou utilisation, vous aurez beaucoup de passages avec ces mots, qui peuvent\n",
    "# ne pas être pertinents pour votre recherche. Donc, l'idéal c'est de combiner les mots clés\n",
    "# importants. Par exemple, on peut associer \"utilisation\" et ..., extraction et données en \n",
    "# \"extraction de données\" pour avoir des passages pertients.\n",
    "# En revance, certains mots clés sont à eux seuls pertients dans la recherche, comme \"scraping\", \n",
    "# \"scraper\", \"robot\" (bon, on suppose qu'on ne va pas parler de robots de cusine dans les cgu),\n",
    "# \"bot\", ...\n",
    "\n",
    "# Enfin, bien que ce programme facilite la recherche d'un mot clés dans les cgu et leur lecture, \n",
    "# il est toujours intéressant de se rapporter sur le lien des cgu que nous affichons dans les réponse\n",
    "# pour de plus amples informations, peut-être sur les passage qui sont affichés. Si vous ne trouvez pas un mot, cela ne veut pas forcément dire qu'il n'exite pas dans les cgu.\n",
    "# peut-être les passages des cgu contenant le mot que vous cherché est inclus dans d'autres balises \n",
    "# différents de ceux qui sont habituellement utilisés pour un texte. D'où l'importance de jetter un coup\n",
    "# d'oeil sur l'url des cgu que nous affichons pour plus d'informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d390a5-bf92-4f20-b39d-3745e91d534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour s'assurer que toutes les pages de l'url sont chargées\n",
    "# \n",
    "async def scroll_to_bottom(page, n):\n",
    "    last_height = await page.evaluate('document.body.scrollHeight')\n",
    "    i = 1\n",
    "\n",
    "    while i <= n:\n",
    "        print(f\"Scrolling page ({i})...\")\n",
    "        # On scrolle jusqu'au bas de la page\n",
    "        await page.evaluate('window.scroll(0, document.body.scrollHeight);')\n",
    "        # On demande au programme d'attendre 1s pour que la page puisse se charger\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "        # On calcul le nouveau scroll (le scroll actuel) et le compare à l'ancien\n",
    "        new_height = await page.evaluate('document.body.scrollHeight')\n",
    "        # On les compare s'ils sont les mêmes alors on sort du boucle\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        last_neight = new_height\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "# async def extract_data(response):\n",
    "#     parsed_url = urlparse(response.url)\n",
    "#     print(\"parsed_url -- \", parsed_url)\n",
    "#     query_params = parse_qs(parsed_url.query)\n",
    "#     print(\"query_params -- \", query_params)\n",
    "#     # On vérifie si 'queryid=products' is in the URL\n",
    "#     if \"queryid\" in query_params and query_params[\"queryid\"][0] == \"products\":\n",
    "#         data = await response.json()\n",
    "#         print(\"JSON data found in : \", response.url)\n",
    "#         print(\"data : \", data)\n",
    "#     else:\n",
    "#         print(\"queryid n'est présent !\")\n",
    "\n",
    "async def extract_data(response):\n",
    "    print(response)\n",
    "    parsed_url = urlparse(response.url)\n",
    "    #print(\"parsed_url -- \", parsed_url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    print(\"query_params -- \", query_params)\n",
    "    #print(\"query_params 0 -- \", query_params[\"key\"][0])\n",
    "\n",
    "    data0 = []\n",
    "    \n",
    "    content_type = response.headers.get(\"content-type\", \"\")\n",
    "    print(\"content-type -- \", content_type)\n",
    "    # # On vérifie si 'queryid=products' is in the URL\n",
    "    #if \"queueittoken\" in query_params or \"api\" in response.url:\n",
    "    #if \"querytype=products\" in response.url.lower():\n",
    "    #if \"queueittoken\" in response.url.lower():\n",
    "    #if \"darty-prod\" in response.url.lower():\n",
    "    # if \"datadome\" in response.url.lower():\n",
    "    #if \"application/json\" in content_type or \"api\" in response.url:\n",
    "    if \"application/json\" in content_type or \"api\" in response.url:\n",
    "        data = await response.json()\n",
    "        data0 = data0.append(data)\n",
    "        print(\"Les données JSON trouvé dans : \", response.url)\n",
    "        print(\"data : \", data)\n",
    "    else:\n",
    "        print(\"Mot cherché non présent !\")\n",
    "\n",
    "    # #if data != {}:\n",
    "    file_name = \"extracted_darty_data.json\"\n",
    "    with open(file_name, 'w', encoding=\"utf-8\") as file:\n",
    "        json.dump(data0, file)\n",
    "    #else:\n",
    "        #print(\"Pas de données enregistrées !\")\n",
    "    \n",
    "    # file_name = \"extracted_data.json\"\n",
    "    # with open(file_name, 'w') as file:\n",
    "    #     json.dump()\n",
    "\n",
    "async def handler():\n",
    "    await page.evaluate(\"window.removeObstructionsForTestIfNeeded()\")\n",
    "    \n",
    "async def scrape_website_product(url):\n",
    "    #ua = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:143.0) Gecko/20100101 Firefox/143.0\"\n",
    "    user_agent = {\"User_Agent\":\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:143.0) Gecko/20100101 Firefox/143.0\"}\n",
    "    ua = UserAgent().random # sans random, il retourn un objet pas un str\n",
    "    print(type(ua), \" -- \", ua)\n",
    "    async with async_playwright() as pw : \n",
    "        # On lance un navigateur\n",
    "        browser = await pw.chromium.launch(headless=False, args=[\"--disable-blink-features=Automation Controlled\"])\n",
    "        #browser = await pw.chromium.launch_persistent_context(user_data_dir=\"/tmp/playwright\", headless=False)\n",
    "        #browser = await pw.firefox.launch(headless=False, args=[\"--start-maximized\"])\n",
    "        #context = await browser.new_context(user_agent=ua)\n",
    "        #context = await browser.new_context(ignore_https_errors=True)\n",
    "        context = await browser.new_context()\n",
    "        await context.set_extra_http_headers({\"User_Agent\":ua})\n",
    "        # On ouvre une page sur cette navigateur\n",
    "        page = await browser.new_page()\n",
    "        #page = await context.new_page()\n",
    "\n",
    "\n",
    "        #await page.add_locator_handler(page.locator(\"body\"), handler, no_wait_after=True)\n",
    "        #await page.get_by_role(\"button\", name=\"Tout accepter\").click()\n",
    "        \n",
    "        # La page écoute et interagit avec l'événement 'response'\n",
    "        page.on(event=\"response\", f=lambda response: extract_data(response))\n",
    "        await asyncio.sleep(1)\n",
    "        # On passe l'url sur sur la barre d'adresse de la page\n",
    "        await page.goto(url, wait_until=\"networkidle\", timeout=0)# wait_until=\"networkidle\", \"domcontentloaded\"\n",
    "        #page.set_default_navigation_timeout(0)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "        # Faire des screeshots\n",
    "        #await page.screenshot(path=url+\".png\", full_page=True)\n",
    "        \n",
    "        # On s'assure que la page est chargé\n",
    "        #await page.evaluate('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        #await scroll_to_bottom(page, 3)\n",
    "        #await page.click(\"text=Voir tout\")\n",
    "\n",
    "        # Pour temu : \n",
    "        # cliquer sur 'Tout accepter'\n",
    "        try : \n",
    "            # button = await page.wait_for_selector('text=Tout accepter')\n",
    "            # #await page.click('text=Tout accepter')\n",
    "            # await button.dispatch_event('click')\n",
    "            # \n",
    "            # print(\"Cookies acceptés\")\n",
    "            # await page.evaluate(\"\"\"\n",
    "            #                 let banners = document.querySelectorAll('[id*=\"cookie\"], \n",
    "            #                 [class*=\"cookie\"], [id*=\"consent\"], [class*=\"consent\"]');\n",
    "            #                 banners.forEach(b => b.remove());\n",
    "            #                 \"\"\")\n",
    "            # print(\"✅ Bandeau supprimé par script\")\n",
    "            # await asyncio.sleep(1)\n",
    "            # await page.mouse.wheel(0, 200) # scroll un peu\n",
    "            # await page.wait_for_timeout(500) # pause\n",
    "            # await page.click(\"text=Tout accepter\", force=True) \n",
    "            \n",
    "            # await context.add_cookies([{\n",
    "            #     \"Name\":\"privacy_setting_detail\",  #cookie_consent\",\n",
    "            #     \"Value\":\"%7B%22firstPAds%22%3A1%2C...11111111011111111111%22%7D\",\n",
    "            #     \"Domain\":\"www.temu.com\",\n",
    "            #     \"HostOnly\":\"true\",\n",
    "            #     \"HttpOnly\":\"false\",\n",
    "            #     \"Path\":\"/\",\n",
    "            #     \"Secure\":\"true\",\n",
    "            #     \"Taille\":\"411\"\n",
    "            # }])\n",
    "\n",
    "            for frame in page.frames:\n",
    "                try:\n",
    "                    #btn = await frame.query_selector(\"text=Tout accepter\")\n",
    "                    btn = await frame.query_selector(\"text=\"+re.search(\".*?\", \"accepter\", re.IGNORECASE).group())\n",
    "                    if btn:\n",
    "                        await btn.click()\n",
    "                        print(\"Cookies acceptés dans un ifrmame\")\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            print(\"Bouton 'Tout accepter' introuvable\")\n",
    "        \n",
    "        # attendre que la page de la promotion s'affiche\n",
    "        # await page.wait_for_selector('.Quitter la promotion')\n",
    "        # await page.click('.Quitter la promotion')\n",
    "\n",
    "        \n",
    "        content = await page.content()\n",
    "        print(content)\n",
    "        #html = response.text()\n",
    "        #print(html[:200])\n",
    "        \n",
    "        \n",
    "        await asyncio.sleep(300)\n",
    "        await context.close()\n",
    "        await browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8440e-75b5-42da-a480-6ab6813155d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2ce93c-e081-44f2-9316-338b99a0f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block page by resource type e.g. image, stylesheet\n",
    "BLOCK_RESOURCE_TYPES = [\n",
    "    'beacon',\n",
    "    'csp_report',\n",
    "    'font',\n",
    "    'image',\n",
    "    'imageset',\n",
    "    'media',\n",
    "    'object',\n",
    "    'texttrack',\n",
    "    # we can even block stylesheets and script through it's not recommended:\n",
    "    # 'stylesheet',\n",
    "    # 'script',\n",
    "    # 'xhr'\n",
    "]\n",
    "\n",
    "# we can also blok popular 3rd party resource like tracking\n",
    "BLOCK_RESOURCE_NAMES = [\n",
    "    'adzerk',\n",
    "    'analytics',\n",
    "    'cdn.api.twitter',\n",
    "    'doubleclick',\n",
    "    'exelator',\n",
    "    'facebook',\n",
    "    'fontawesome',\n",
    "    'google',\n",
    "    'google-analytics',\n",
    "    'googletagmanager',\n",
    "]\n",
    "\n",
    "async def intercept_route(route):\n",
    "    \"\"\" intercept all request and abort blocked ones \"\"\" \n",
    "    if route.request.reource_type in BLOCK_RESOURCE_TYPES:\n",
    "        print(f'blocking background resource {route.resource} blocked type \"{route.request.reource_type}\"') \n",
    "        return route.abort()\n",
    "    if any(key in route.request.url for key in BLOCK_RESOURCE_NAMES):\n",
    "        print(f\"blocking backgroud resource {route.resource} blocked name {route.request.url}\")\n",
    "        return route.abort()\n",
    "\n",
    "    return route.continue_()\n",
    "\n",
    "# Scrolling et loading...\n",
    "async def scroll_to_bottom_and_load(page, n, text_to_click):\n",
    "    last_height = await page.evaluate('document.body.scrollHeight')\n",
    "    i = 1\n",
    "\n",
    "    while i <= n:\n",
    "        print(f'Page {i}...')\n",
    "        # du début jusquà une position donnée\n",
    "        await page.evaluate('window.scroll(0, document.body.scrollHeight)')\n",
    "        asyncio.sleep(1)\n",
    "        \n",
    "        # position actuelle\n",
    "        new_height = await page.evaluate('document.body.scrollHeight')\n",
    "\n",
    "        if new_height == last_height:\n",
    "            try: \n",
    "                await page.wait_for_selector(\"text=\"+text_to_click)\n",
    "                await page.click(\"text=\"+text_to_click)\n",
    "            except TimeoutError:\n",
    "                print('Limite atteinte.')\n",
    "                break\n",
    "        # On réinitialise last_height pour la page suivante\n",
    "        last_height = new_height        \n",
    "        i+=1\n",
    "\n",
    "async def website_scraper(url): #, text_bandeau=\"\", selector):\n",
    "    async with async_playwright() as pw:\n",
    "        # create browser instance\n",
    "        browser = await pw.chromium.launch(\n",
    "            # we can choose either headful (with GUI) or headless mode:\n",
    "            headless=False,\n",
    "            #devtools=True\n",
    "            args=['--disable-blink-']\n",
    "        )\n",
    "        # create context\n",
    "        # Using context we can define page properties like viewport dimensions\n",
    "        context = await browser.new_context(\n",
    "            # most commun desktop viewport is 1920x1080\n",
    "            viewport={\"width\":1920, \"height\":1080}\n",
    "        )\n",
    "        # create page aka browser tab wich we'll be using to do everything\n",
    "        page = await context.new_page()\n",
    "\n",
    "        # enable for intercepting for this page, **/* enable for all request\n",
    "        page.on(\"**/*\", intercept_route)\n",
    "        \n",
    "        # go to url\n",
    "        #await page.goto(\"https://twitch.tv/directory/game/Art\")\n",
    "        await page.goto(url, timeout=0)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "        await page.wait_for_selector(\"text=J'ACCEPTE\", timeout=0)\n",
    "        await page.click(\"text=J'ACCEPTE\")\n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "        # get html\n",
    "        #print(await page.content())\n",
    "        # wait for first result appear\n",
    "        page.wait_for_selector(\"body\", timeout=0)\n",
    "        asyncio.sleep(1)\n",
    "        \n",
    "        await scroll_to_bottom_and_load(page, 6, \"Voir plus de produits\")\n",
    "        asyncio.sleep(1)\n",
    "       \n",
    "        products = page.locator(\".product\")\n",
    "        count = await products.count()\n",
    "        print(\"nombre de produits : \", count)\n",
    "\n",
    "        products_list = []\n",
    "\n",
    "        for i in range(count):\n",
    "            family = await products.nth(i).locator(\".product .column.center .family\").inner_text()\n",
    "            brand = await products.nth(i).locator(\".product .column.center .brand\").inner_text()\n",
    "            reference = await products.nth(i).locator(\".product .column.center .reference\").inner_text()\n",
    "            average = await products.nth(i).locator(\".product .column.center .rating .average\").inner_text()\n",
    "            reviewNumber = await products.nth(i).locator(\".product .column.center .rating .count .deco\").inner_text()\n",
    "            seller = await products.nth(i).locator(\".product .column.right .seller\").inner_text()\n",
    "            label = await products.nth(i).locator(\".product .column.right .seller .label\").inner_text()\n",
    "            price_ttc = await products.nth(i).locator(\".product .column.right .price_product .price.price_ttc\").inner_text()\n",
    "            \n",
    "            products_list.append({\"typeTelephone\":family,\n",
    "                                 \"marque\":brand,\n",
    "                                 \"reference\":reference,\n",
    "                                 \"note\":average,\n",
    "                                 \"nombreAvis\":reviewNumber,\n",
    "                                 \"vendeur\":seller,\n",
    "                                 \"nomDuVendeur\":label,\n",
    "                                 \"prix\":price_ttc})\n",
    "            print(f\"family : {family}\\n brand : {brand}\\n reference : {reference}\\n\"+\n",
    "                  f\"average : {average}\\n deco : {reviewNumber}\\n seller : {seller}\\n\"+\n",
    "                  f\"label : {label}\\n price_ttc : {price_ttc}\")\n",
    "            \n",
    "            \n",
    "        with open(\"darty_html.csv\", 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "            fieldnames = products_list[0].keys()#[\"marque\", \"année\", \"prix\"]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(products_list)\n",
    "\n",
    "        # await asyncio.sleep(300)\n",
    "        # await context.close()\n",
    "        # await browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc5d00c6-9945-4f77-970e-9805c1dcc321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Instanciation des variables\n",
    "\"\"\"\n",
    "#  Paramètres\n",
    "# lire des mots clés sur un lien\n",
    "\n",
    "robots_keywords = [\"product\", \"products\", \"item\", \"itm\", \"kw\", \"produit\", \"produits\", \"listing\", \"dp\", \"catalog\", \"catalogue\"]\n",
    "base_url = \"https://www.fnac.com\" # \"https://www.ebay.com\", \"https://www.amazon.com\"\n",
    "\n",
    "\n",
    "# Liste de probables mots qui peuvent se trouver dans les titres des cgu\n",
    "cgu_keywords_list1 = [\"interdit\", \"robots\", \"bots\", \"scraper\", \"scraping\", \"aspirer\", \"logiciels\", \"automatisé\", \n",
    "                         \"aspiration\", \"extraction de données\", \"automated access\", \"crawling\", \n",
    "                         \"data extraction\", \"spider\", \"personnel\", \"commercial\", \"partie substantielle\", \"partie non susbtantielle\",\n",
    "                         \"usage\", \"licence\", \"propriété\", \"utilisation\", \"API\"]\n",
    "cgu_keywords_list = [\"interdit\", \"robots\", \"bots\", \"scraper\", \"scraping\", \"aspiration de données\", \n",
    "                     \"logiciels automatisés\", \"extraction de données\", \"automated access\", \n",
    "                     \"crawling\", \"data extraction\", \"spider\", \"usage personnel\", \n",
    "                     \"usage commercial\", \"non commercial\", \"partie substantielle\", \n",
    "                     \"partie non susbtantielle\", \"API\"]\n",
    "\n",
    "darty_url, darty_cgu_path = \"https://cuisine.darty.com\", \"/cgu\" #\"https://cadeau.darty-services.com\", \"/CGU.pdf\"\n",
    "fnac_url, fnac_cgu_path = \"https://www.fnac.com\", \"/Help/cgu-fnac\"\n",
    "amazon_url, amazon_cgu_path = \"https://www.amazon.com\", \"/gp/help/customer/display.html?nodeId=GLSBYFE9MGKKQXXM\"\n",
    "ebay_url, ebay_cgu_path = \"https://www.ebay.fr\", \"/help/policies/member-behaviour-policies/user-agreement?id=4259\"\n",
    "off_url, off_cgu_path = \"https://fr.openfoodfacts.org\", \"/conditions-d-utilisation\"\n",
    "etsy_url, etsy_cgu_path = \"https://www.etsy.com\", \"/fr/legal/terms-of-use\"\n",
    "temu_url, temu_cgu_path = \"https://www.temu.com\", \"/fr/terms-of-use.html?refer_page_name=support-center&refer_page_id=10318_1758844934157_ynijdhm7h4&refer_page_sn=10318&_x_sessn_id=s1w3r2xioe\"\n",
    "bts_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "#off_produits_url = \"https://fr.openfoodfacts.org/cgi/search.pl?search_terms=produi&search_simple=1&action=process\"\n",
    "off_produits_url = \"https://fr.openfoodfacts.org/produit/\"\n",
    "sts_items_url = \"https://www.scrapethissite.com/pages/simple/\"\n",
    "nike_products_url = \"https://www.nike.com/cz/en/w/mens-shoes-nik1zy7ok\" #\"https://www.nike.com/cz/en/men\" #\"https://www.nike.com/cz/en/t/air-max-plus-shoes-EWa8uypC/\" #\"https://www.nike.com/cz/en/w/mens-air-max-lifestyle-shoes-13jrmza6d8hznik1zy7ok\"\n",
    "darty1_products_url = \"https://darty.com/nav/achat/informatique/index.html\"\n",
    "darty_products_url = \"https://darty.com/nav/extra/list?flag=ODR&cat=89054&prix_barre=dcom_BonPlan-dcom_BONPLAN-dcom_OFDARTY&fa=294580-693-138552\"\n",
    "temu_products_url = \"https://www.temu.com/fr/cell-phones-o3-2641.html?opt_level=2&title=T%C3%A9l%C3%A9phones%20portables&_x_enter_scene_type=cate_tab&leaf_type=bro&show_search_type=3&_x_sessn_id=hee7ujhfgf&refer_page_name=category&refer_page_id=10012_1759272164200_73ttyjc3sc&refer_page_sn=10012\"\n",
    "\n",
    "\n",
    "# base_url+\"robots.txt\"\n",
    "\n",
    "# darty -> ok, usage de partie non substantiel\n",
    "# temu -> ok, usage personnel et non commercial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e5322d8-64b8-4149-8e7f-e600c8422b17",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "TargetClosedError",
     "evalue": "Page.wait_for_selector: Target page, context or browser has been closed\nCall log:\n  - waiting for locator(\"text=J'ACCEPTE\") to be visible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTargetClosedError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m    Appel des fonctions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Main\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#robots_results(base_url, robots_keywords);\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#await scrape_website_product(temu_products_url) #\"https://www.temu.com/\") #darty_products_url) # temu_products_url\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m website_scraper(darty_products_url)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# \"usage personnel ou non commercial\", \"logiciels manuels ou automatisés\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# soup = BeautifulSoup(page, \"html.parser\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mChrome/135.0.0.0 Safari/537.36\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m        }\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 92\u001b[0m, in \u001b[0;36mwebsite_scraper\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mgoto(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mwait_for_selector(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext=J\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mACCEPTE\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mclick(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext=J\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mACCEPTE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/playwright/async_api/_generated.py:8181\u001b[0m, in \u001b[0;36mPage.wait_for_selector\u001b[0;34m(self, selector, timeout, state, strict)\u001b[0m\n\u001b[1;32m   8110\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_selector\u001b[39m(\n\u001b[1;32m   8111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8112\u001b[0m     selector: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8118\u001b[0m     strict: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   8119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElementHandle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   8120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Page.wait_for_selector\u001b[39;00m\n\u001b[1;32m   8121\u001b[0m \n\u001b[1;32m   8122\u001b[0m \u001b[38;5;124;03m    Returns when element specified by selector satisfies `state` option. Returns `null` if waiting for `hidden` or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8177\u001b[0m \u001b[38;5;124;03m    Union[ElementHandle, None]\u001b[39;00m\n\u001b[1;32m   8178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   8180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_impl_nullable(\n\u001b[0;32m-> 8181\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mwait_for_selector(\n\u001b[1;32m   8182\u001b[0m             selector\u001b[38;5;241m=\u001b[39mselector, timeout\u001b[38;5;241m=\u001b[39mtimeout, state\u001b[38;5;241m=\u001b[39mstate, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m   8183\u001b[0m         )\n\u001b[1;32m   8184\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/playwright/_impl/_page.py:423\u001b[0m, in \u001b[0;36mPage.wait_for_selector\u001b[0;34m(self, selector, timeout, state, strict)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_selector\u001b[39m(\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    418\u001b[0m     selector: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[ElementHandle]:\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main_frame\u001b[38;5;241m.\u001b[39mwait_for_selector(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/playwright/_impl/_frame.py:369\u001b[0m, in \u001b[0;36mFrame.wait_for_selector\u001b[0;34m(self, selector, strict, timeout, state)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_selector\u001b[39m(\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    363\u001b[0m     selector: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m     state: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattached\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetached\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisible\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    367\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[ElementHandle]:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_nullable_channel(\n\u001b[0;32m--> 369\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    370\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaitForSelector\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout, locals_to_params(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m    371\u001b[0m         )\n\u001b[1;32m    372\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/playwright/_impl/_connection.py:69\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, timeout_calculator, params, is_internal, title)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     63\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     title: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_send(method, timeout_calculator, params, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     71\u001b[0m         is_internal,\n\u001b[1;32m     72\u001b[0m         title,\n\u001b[1;32m     73\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/playwright/_impl/_connection.py:558\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal, title)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_zone\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTargetClosedError\u001b[0m: Page.wait_for_selector: Target page, context or browser has been closed\nCall log:\n  - waiting for locator(\"text=J'ACCEPTE\") to be visible\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Appel des fonctions\n",
    "\"\"\"\n",
    "if __name__=='__main__':\n",
    "    # Main\n",
    "    #robots_results(base_url, robots_keywords);\n",
    "    #robots_keywords = robots_file_analysis(base_url, robots_keywords) \n",
    "    # pour éviter l'affichage du retur, on peut la fonction d'appel dans une variable ou mettre un ; à la fin\n",
    "    #display_robots_results(base_url, robots_keywords)\n",
    "\n",
    "    #cgu_analysis(url_fnac, fnac_cgu_path)\n",
    "    #await cgu_analysis_auto(url_temu, temu_cgu_path, cgu_keywords_list) # url_fnac, fnac_cgu_path\n",
    "    \n",
    "    #await display_cgu_results(url_off, off_cgu_path, cgu_keywords_list)\n",
    "\n",
    "    #await scrape_website_product(temu_products_url) #\"https://www.temu.com/\") #darty_products_url) # temu_products_url\n",
    "\n",
    "    await website_scraper(darty_products_url)\n",
    "    \n",
    "    # \"usage personnel ou non commercial\", \"logiciels manuels ou automatisés\"\n",
    "    # soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "    \"\"\"\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) \n",
    "Chrome/135.0.0.0 Safari/537.36\"\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \n",
    "Chrome/135.0.0.0 Safari/537.36\"\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) \n",
    "Version/16.4 Mobile/15E148 Safari/604.1\"\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \n",
    "Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0\"\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 18_3_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like \n",
    "Gecko) Version/18.3.1 Mobile/15E148 Safari/604.1\"\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 \n",
    "Safari/537.36\"\n",
    "    \n",
    "        query_params -- \n",
    "{\n",
    "    'flag': ['ODR'],\n",
    "    'cat': ['89054'],\n",
    "    'prix_barre': ['dcom_BonPlan-dcom_BONPLAN-dcom_OFDARTY'],\n",
    "    'fa': ['294580-693-138552']\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "# class=\"text ellipsis ellipsis-2lines\n",
    "# <span class=\"trigger-text ellipsis ellipsis-2lines\">Caractéristiques</span>\n",
    "# <span class=\"average\">4,7/5</span>\n",
    "# <span class=\"deco\">16 avis</span>\n",
    "# <span class=\"family\">Smartphone</span>\n",
    "# <span class=\"brand\">Samsung</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad42547-8ea3-4655-bb7c-fb7ac8b2a117",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2280c-6b54-4255-bc91-4a0a5e25491f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a7c582e-d7eb-436a-85d6-cefef265307f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Erreurs\n",
    "\n",
    "# TypeError: object of type 'Page' has no len()\n",
    "# /tmp/ipykernel_121818/2292587623.py:29: RuntimeWarning: coroutine 'Locator.all' was never awaited\n",
    "#   for row in page.get_by_role(\"listitem\").all():\n",
    "# RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
    "\n",
    "# TimeoutError: Page.goto: Timeout 30000ms exceeded.\n",
    "# Call log:\n",
    "#   - navigating to \"https://www.ebay.fr/help/policies/member-behaviour-policies/user-agreement?id=4259\", waiting until \"networkidle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb792a09-b5e1-4d69-b89c-97053e36ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'url du site sur lequel \n",
    "url = \"\"\n",
    "\n",
    "# fonction qui l'url et renvoi un dataFrame\n",
    "def scraper_site(url) : \n",
    "    url = url\n",
    "    \n",
    "    # Créer un objet browser, pour se placer sur le navigateur\n",
    "    browser = mechanicalsoup.Browser()\n",
    "    # Ouverture de la page web de l'url sur le navigateur\n",
    "    page = browser.get(url)\n",
    "    # Récupérer la page html avec l'objet soup de Beautiful\n",
    "    html = page.soup\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85e0a2d2-7688-41f3-97d3-17d5c887b354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmarkup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbuilder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mparse_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfrom_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexclude_encodings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A data structure representing a parsed HTML or XML document.\n",
       "\n",
       "Most of the methods you'll call on a BeautifulSoup object are inherited from\n",
       "PageElement or Tag.\n",
       "\n",
       "Internally, this class defines the basic interface called by the\n",
       "tree builders when converting an HTML/XML document into a data\n",
       "structure. The interface abstracts away the differences between\n",
       "parsers. To write a new tree builder, you'll need to understand\n",
       "these methods as a whole.\n",
       "\n",
       "These methods will be called by the BeautifulSoup constructor:\n",
       "  * reset()\n",
       "  * feed(markup)\n",
       "\n",
       "The tree builder may call these methods from its feed() implementation:\n",
       "  * handle_starttag(name, attrs) # See note about return value\n",
       "  * handle_endtag(name)\n",
       "  * handle_data(data) # Appends to the current data node\n",
       "  * endData(containerClass) # Ends the current data node\n",
       "\n",
       "No matter how complicated the underlying parser is, you should be\n",
       "able to build a tree using 'start tag' events, 'end tag' events,\n",
       "'data' events, and \"done with data\" events.\n",
       "\n",
       "If you encounter an empty-element tag (aka a self-closing tag,\n",
       "like HTML's <br> tag), call handle_starttag and then\n",
       "handle_endtag.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Constructor.\n",
       "\n",
       ":param markup: A string or a file-like object representing\n",
       " markup to be parsed.\n",
       "\n",
       ":param features: Desirable features of the parser to be\n",
       " used. This may be the name of a specific parser (\"lxml\",\n",
       " \"lxml-xml\", \"html.parser\", or \"html5lib\") or it may be the\n",
       " type of markup to be used (\"html\", \"html5\", \"xml\"). It's\n",
       " recommended that you name a specific parser, so that\n",
       " Beautiful Soup gives you the same results across platforms\n",
       " and virtual environments.\n",
       "\n",
       ":param builder: A TreeBuilder subclass to instantiate (or\n",
       " instance to use) instead of looking one up based on\n",
       " `features`. You only need to use this if you've implemented a\n",
       " custom TreeBuilder.\n",
       "\n",
       ":param parse_only: A SoupStrainer. Only parts of the document\n",
       " matching the SoupStrainer will be considered. This is useful\n",
       " when parsing part of a document that would otherwise be too\n",
       " large to fit into memory.\n",
       "\n",
       ":param from_encoding: A string indicating the encoding of the\n",
       " document to be parsed. Pass this in if Beautiful Soup is\n",
       " guessing wrongly about the document's encoding.\n",
       "\n",
       ":param exclude_encodings: A list of strings indicating\n",
       " encodings known to be wrong. Pass this in if you don't know\n",
       " the document's encoding but you know Beautiful Soup's guess is\n",
       " wrong.\n",
       "\n",
       ":param element_classes: A dictionary mapping BeautifulSoup\n",
       " classes like Tag and NavigableString, to other classes you'd\n",
       " like to be instantiated instead as the parse tree is\n",
       " built. This is useful for subclassing Tag or NavigableString\n",
       " to modify default behavior.\n",
       "\n",
       ":param kwargs: For backwards compatibility purposes, the\n",
       " constructor accepts certain keyword arguments used in\n",
       " Beautiful Soup 3. None of these arguments do anything in\n",
       " Beautiful Soup 4; they will result in a warning and then be\n",
       " ignored.\n",
       " \n",
       " Apart from this, any keyword arguments passed into the\n",
       " BeautifulSoup constructor are propagated to the TreeBuilder\n",
       " constructor. This makes it possible to configure a\n",
       " TreeBuilder by passing in arguments, not just by saying which\n",
       " one to use.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.11/site-packages/bs4/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     BeautifulStoneSoup"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#help(requests)\n",
    "BeautifulSoup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de31d85-1ef7-41ba-8cf9-3fc19c3756e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833c343-3435-4808-af5f-601e88b81875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3111f-a0c6-4845-b25b-96c2eeadca02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773159d-f121-4d7c-b573-ff429c2aa2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
